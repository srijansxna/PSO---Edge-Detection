{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp3bC70IaiKP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_Vu0MN8cKhV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import zipfile, os, shutil\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J0R-7poeXGZ",
        "outputId": "7951aa42-f709-4d4b-c256-c028d8346a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.Mounting Google Drive...\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "print(\"1.Mounting Google Drive...\")\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuYXUwo6fAvB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbaSTgqbe0ES"
      },
      "outputs": [],
      "source": [
        "zip_path = \"/content/drive/MyDrive/archive.zip\"\n",
        "extract_dir = \"/content/chest_xray\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJeG1Irve4Qg",
        "outputId": "5a102089-b0dc-44b6-9cc9-1ee1d1443ba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2. Extracting /content/drive/MyDrive/archive.zip...\n",
            "Extraction successful.\n"
          ]
        }
      ],
      "source": [
        "print(f\"2. Extracting {zip_path}...\")\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(\"Extraction successful.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: ZIP file not found at {zip_path}. Please check the path.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbzLW_kzfcH8",
        "outputId": "990aec6c-a5b4-4aab-8caf-ab9e5653fd19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset base directory found at: /content/chest_xray/chest_xray\n"
          ]
        }
      ],
      "source": [
        "data_dir = None\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    if \"train\" in dirs:\n",
        "        data_dir = root\n",
        "        break\n",
        "if data_dir is None:\n",
        "    data_dir = extract_dir\n",
        "print(\"Dataset base directory found at:\", data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOpnokAofl57",
        "outputId": "df00b1c6-fd80-4217-83dd-d40bce0f67cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3.  Preparing Data Loaders...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n3.  Preparing Data Loaders...\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6qYEE3Afoky"
      },
      "outputs": [],
      "source": [
        "train_path = os.path.join(data_dir, \"train\")\n",
        "try:\n",
        "    dataset = ImageFolder(root=train_path, transform=transform)\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "except RuntimeError as e:\n",
        "     print(f\" Error loading data from {train_path}. Check if 'train' subfolder exists and contains images.\")\n",
        "     print(e)\n",
        "     exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o5-WjKRqfseQ",
        "outputId": "6dabc69d-952c-4a02-aacc-33b93990db81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4. Extracting features using ResNet18...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 122MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Identity()\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"4. Extracting features using ResNet18...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "resnet.fc = nn.Identity()  # remove final classification layer\n",
        "resnet = resnet.to(device)\n",
        "resnet.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqgvV9v9fzDn"
      },
      "outputs": [],
      "source": [
        "features, labels = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCvDkm-9f2Xc",
        "outputId": "47a2b6a4-00aa-4afb-891f-b00fb5cfce29"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ” Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [01:35<00:00,  1.70it/s]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    for imgs, lbls in tqdm(train_loader, desc=\"ðŸ” Feature Extraction Progress\"):\n",
        "        imgs = imgs.to(device)\n",
        "        out = resnet(imgs).cpu().numpy()\n",
        "        features.append(out)\n",
        "        labels.append(lbls.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKbrEHCnf5Ne",
        "outputId": "1b1587be-a024-4e81-862a-fd5bbeca5354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction complete! Shape: (5216, 512)\n"
          ]
        }
      ],
      "source": [
        "features = np.concatenate(features)\n",
        "labels = np.concatenate(labels)\n",
        "print(\"Feature extraction complete! Shape:\", features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi8fmuvUgC6c",
        "outputId": "d128bc27-d9b1-42d3-db67-da0f31bd0c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data split: Train=3912, Test=1304. Total features: 512\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=42)\n",
        "N_FEATURES = features.shape[1]\n",
        "print(f\"Data split: Train={len(X_train)}, Test={len(X_test)}. Total features: {N_FEATURES}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w33cYaT2lqvF",
        "outputId": "7b54d3e4-c196-4eba-96fc-744b7e36cd84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Starting QUICK BPSO Test Run: 10 Iterations\n",
            "Parameters: Swarm=50, w=0.7, c1=1.7, c2=1.3\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations:  10%|â–ˆ         | 1/10 [00:13<01:57, 13.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  1/10 | Best Acc: 0.9816 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations:  20%|â–ˆâ–ˆ        | 2/10 [00:26<01:46, 13.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  2/10 | Best Acc: 0.9816 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:41<01:37, 13.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  3/10 | Best Acc: 0.9816 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:54<01:21, 13.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  4/10 | Best Acc: 0.9816 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:07<01:06, 13.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  5/10 | Best Acc: 0.9839 | Features: 272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:20<00:52, 13.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  6/10 | Best Acc: 0.9839 | Features: 272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:33<00:39, 13.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  7/10 | Best Acc: 0.9839 | Features: 272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:46<00:26, 13.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  8/10 | Best Acc: 0.9839 | Features: 272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:58<00:12, 12.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  9/10 | Best Acc: 0.9839 | Features: 272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Quick Iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:11<00:00, 13.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter 10/10 | Best Acc: 0.9839 | Features: 272\n",
            "\n",
            "Quick BPSO Run Complete (10 Iterations)!\n",
            "Final Test Accuracy: 0.9839\n",
            "Features Selected: 272 / 512\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Binary Particle Swarm Optimization (BPSO) - 10 ITERATION TEST\n",
        "# ==============================\n",
        "\n",
        "# --- Custom Hyperparameters for Quick Test ---\n",
        "QUICK_N_ITERATIONS = 10  # Quick run limit\n",
        "QUICK_N_PARTICLES = 50   # Swarm Size: A bit smaller for speed (50 instead of 100)\n",
        "QUICK_W = 0.7            # Inertia weight: Kept at 0.7 for balance\n",
        "QUICK_C1 = 1.7           # Cognitive coefficient\n",
        "QUICK_C2 = 1.3           # Social coefficient\n",
        "QUICK_V_MAX = 6.0        # Max velocity\n",
        "\n",
        "# --- Initialization ---\n",
        "N_FEATURES = features.shape[1] # Assumes 'features' is defined from step 4\n",
        "positions = np.random.randint(0, 2, size=(QUICK_N_PARTICLES, N_FEATURES))\n",
        "velocities = np.zeros((QUICK_N_PARTICLES, N_FEATURES))\n",
        "pbest_positions = positions.copy()\n",
        "pbest_fitness = np.full(QUICK_N_PARTICLES, -np.inf)\n",
        "gbest_position = np.random.randint(0, 2, size=N_FEATURES)\n",
        "gbest_fitness = -np.inf\n",
        "\n",
        "# --- Evaluation Function (Reuse the function defined previously) ---\n",
        "# NOTE: Ensure the evaluate_subset_accuracy function (and X_train/X_test/y_train/y_test)\n",
        "# from the main script is accessible in the Colab session.\n",
        "def evaluate_subset_accuracy(individual, X_data_train, X_data_test, y_data_train, y_data_test):\n",
        "    \"\"\"Evaluates the fitness (accuracy) of a feature subset.\"\"\"\n",
        "    selected = [i for i, bit in enumerate(individual) if bit == 1]\n",
        "    if len(selected) == 0:\n",
        "        return 0.0, 0\n",
        "\n",
        "    X_sel = X_data_train[:, selected]\n",
        "    X_tst = X_data_test[:, selected]\n",
        "\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    clf = LogisticRegression(max_iter=300, random_state=42, solver='liblinear')\n",
        "    clf.fit(X_sel, y_data_train)\n",
        "    preds = clf.predict(X_tst)\n",
        "    acc = accuracy_score(y_data_test, preds)\n",
        "    return acc, len(selected)\n",
        "\n",
        "\n",
        "# --- BPSO Main Loop (10 Iterations) ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"Starting QUICK BPSO Test Run: {QUICK_N_ITERATIONS} Iterations\")\n",
        "print(f\"Parameters: Swarm={QUICK_N_PARTICLES}, w={QUICK_W}, c1={QUICK_C1}, c2={QUICK_C2}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for iteration in tqdm(range(QUICK_N_ITERATIONS), desc=\"PSO Quick Iterations\"):\n",
        "    # 1. Evaluate Fitness & Update Bests\n",
        "    for i in range(QUICK_N_PARTICLES):\n",
        "        acc, _ = evaluate_subset_accuracy(positions[i], X_train, X_test, y_train, y_test)\n",
        "\n",
        "        # Update Personal Best (pbest)\n",
        "        if acc > pbest_fitness[i]:\n",
        "            pbest_fitness[i] = acc\n",
        "            pbest_positions[i] = positions[i].copy()\n",
        "\n",
        "            # Update Global Best (gbest)\n",
        "            if acc > gbest_fitness:\n",
        "                gbest_fitness = acc\n",
        "                gbest_position = positions[i].copy()\n",
        "\n",
        "    # 2. Update Velocity and Position\n",
        "    for i in range(QUICK_N_PARTICLES):\n",
        "        r1 = np.random.rand(N_FEATURES)\n",
        "        r2 = np.random.rand(N_FEATURES)\n",
        "\n",
        "        # Velocity Update\n",
        "        cognitive_vel = QUICK_C1 * r1 * (pbest_positions[i] - positions[i])\n",
        "        social_vel = QUICK_C2 * r2 * (gbest_position - positions[i])\n",
        "        velocities[i] = QUICK_W * velocities[i] + cognitive_vel + social_vel\n",
        "        velocities[i] = np.clip(velocities[i], -QUICK_V_MAX, QUICK_V_MAX)\n",
        "\n",
        "        # Position Update (Binary Sigmoid)\n",
        "        prob = 1.0 / (1.0 + np.exp(-velocities[i]))\n",
        "        rand_nums = np.random.rand(N_FEATURES)\n",
        "        positions[i] = (rand_nums < prob).astype(int)\n",
        "\n",
        "    selected_count = int(np.sum(gbest_position))\n",
        "    tqdm.write(f\"  Iter {iteration+1:2d}/{QUICK_N_ITERATIONS} | Best Acc: {gbest_fitness:.4f} | Features: {selected_count}\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Final Results for Quick Run\n",
        "# ==============================\n",
        "final_acc, final_features_count = evaluate_subset_accuracy(gbest_position, X_train, X_test, y_train, y_test)\n",
        "\n",
        "print(\"\\nQuick BPSO Run Complete (10 Iterations)!\")\n",
        "print(f\"Final Test Accuracy: {final_acc:.4f}\")\n",
        "print(f\"Features Selected: {final_features_count} / {N_FEATURES}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "APbiKAmLg-az",
        "outputId": "e92e9a5d-9700-48ff-bb78-33ca81365719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Starting experiment for PSO Iterations = 25\n",
            "Hyperparameters: Swarm=100, w=0.7, c1=1.7, c2=1.3\n",
            "======================================================================\n",
            "\n",
            "ðŸš€ Starting BPSO for 25 Iterations...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [04:14<06:18, 25.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  10/25 | Best Acc: 0.9839 | Features: 238\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [08:23<02:04, 24.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  20/25 | Best Acc: 0.9847 | Features: 245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [10:28<00:00, 25.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  25/25 | Best Acc: 0.9847 | Features: 245\n",
            "\n",
            "BPSO Run Complete (25 Iterations)!\n",
            "Final Test Accuracy: 0.9847\n",
            " Features Selected: 245 / 512\n",
            "\n",
            "======================================================================\n",
            "Starting experiment for PSO Iterations = 50\n",
            "Hyperparameters: Swarm=100, w=0.7, c1=1.7, c2=1.3\n",
            "======================================================================\n",
            "\n",
            "ðŸš€ Starting BPSO for 50 Iterations...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  20%|â–ˆâ–ˆ        | 10/50 [04:18<17:17, 25.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  10/50 | Best Acc: 0.9824 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [08:37<12:56, 25.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  20/50 | Best Acc: 0.9854 | Features: 270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [12:55<08:39, 25.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  30/50 | Best Acc: 0.9854 | Features: 270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [17:13<04:15, 25.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  40/50 | Best Acc: 0.9862 | Features: 251\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [21:26<00:00, 25.74s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  50/50 | Best Acc: 0.9862 | Features: 251\n",
            "\n",
            "BPSO Run Complete (50 Iterations)!\n",
            "Final Test Accuracy: 0.9862\n",
            " Features Selected: 251 / 512\n",
            "\n",
            "======================================================================\n",
            "Starting experiment for PSO Iterations = 100\n",
            "Hyperparameters: Swarm=100, w=0.7, c1=1.7, c2=1.3\n",
            "======================================================================\n",
            "\n",
            "ðŸš€ Starting BPSO for 100 Iterations...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  10%|â–ˆ         | 10/100 [04:15<38:32, 25.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  10/100 | Best Acc: 0.9839 | Features: 254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  20%|â–ˆâ–ˆ        | 20/100 [08:28<34:02, 25.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  20/100 | Best Acc: 0.9847 | Features: 255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [12:47<30:20, 26.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  30/100 | Best Acc: 0.9862 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [17:06<25:47, 25.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  40/100 | Best Acc: 0.9862 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [21:22<21:21, 25.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  50/100 | Best Acc: 0.9862 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [25:37<17:02, 25.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  60/100 | Best Acc: 0.9862 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [29:52<12:44, 25.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  70/100 | Best Acc: 0.9862 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [34:13<08:39, 26.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  80/100 | Best Acc: 0.9862 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [38:31<04:17, 25.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter  90/100 | Best Acc: 0.9862 | Features: 269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PSO Iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [42:48<00:00, 25.68s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Iter 100/100 | Best Acc: 0.9862 | Features: 269\n",
            "\n",
            "BPSO Run Complete (100 Iterations)!\n",
            "Final Test Accuracy: 0.9862\n",
            " Features Selected: 269 / 512\n",
            "\n",
            "######################################################################\n",
            "### ðŸ† Final Comparison of PSO Convergence Across Iterations ###\n",
            "######################################################################\n",
            "     â–¶25 Iterations:\n",
            "     Accuracy: 0.9847\n",
            "     Features: 245 / 512\n",
            "\n",
            "     â–¶50 Iterations:\n",
            "     Accuracy: 0.9862\n",
            "     Features: 251 / 512\n",
            "\n",
            "     â–¶100 Iterations:\n",
            "     Accuracy: 0.9862\n",
            "     Features: 269 / 512\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# ðŸ§¬ Binary Particle Swarm Optimization (BPSO) Core - **REVISED**\n",
        "# ==============================\n",
        "\n",
        "# --- PSO Hyperparameters (Based on your Summary Table) ---\n",
        "N_PARTICLES = 100   # Swarm Size: Increased for a more robust search!\n",
        "W = 0.7             # Inertia weight: Balances global and local search\n",
        "C1 = 1.7            # Cognitive coefficient: Attraction to personal best (pbest)\n",
        "C2 = 1.3            # Social coefficient: Attraction to global best (gbest)\n",
        "V_MAX = 6.0         # Maximum velocity (Kept for stability in binary PSO)\n",
        "\n",
        "# --- Evaluation Function (Remains the same, but defined again for completeness) ---\n",
        "def evaluate_subset_accuracy(individual, X_data_train, X_data_test, y_data_train, y_data_test):\n",
        "    \"\"\"Evaluates the fitness (accuracy) of a feature subset.\"\"\"\n",
        "    selected = [i for i, bit in enumerate(individual) if bit == 1]\n",
        "    if len(selected) == 0:\n",
        "        return 0.0, 0 # Return accuracy and feature count\n",
        "\n",
        "    # Select features\n",
        "    X_sel = X_data_train[:, selected]\n",
        "    X_tst = X_data_test[:, selected]\n",
        "\n",
        "    # Train and evaluate a simple classifier\n",
        "    # solver='liblinear' is generally good for small datasets and binary classification\n",
        "    # Max_iter is kept at 300 for faster evaluation within the inner loop\n",
        "    clf = LogisticRegression(max_iter=300, random_state=42, solver='liblinear')\n",
        "    clf.fit(X_sel, y_data_train)\n",
        "    preds = clf.predict(X_tst)\n",
        "    acc = accuracy_score(y_data_test, preds)\n",
        "    return acc, len(selected)\n",
        "\n",
        "\n",
        "# --- BPSO Runner Function ---\n",
        "def run_bpso(n_iterations, X_train, X_test, y_train, y_test, n_features):\n",
        "    \"\"\"Initializes and runs the BPSO algorithm for a given number of iterations.\"\"\"\n",
        "\n",
        "    # Initialization\n",
        "    # Initialize positions (particles) randomly\n",
        "    positions = np.random.randint(0, 2, size=(N_PARTICLES, n_features))\n",
        "    velocities = np.zeros((N_PARTICLES, n_features))\n",
        "\n",
        "    # Initialize personal bests (pbest)\n",
        "    pbest_positions = positions.copy()\n",
        "    pbest_fitness = np.full(N_PARTICLES, -np.inf)\n",
        "\n",
        "    # Initialize global best (gbest)\n",
        "    gbest_position = np.random.randint(0, 2, size=n_features)\n",
        "    gbest_fitness = -np.inf\n",
        "\n",
        "    # Storage for tracking results\n",
        "    history = []\n",
        "\n",
        "    print(f\"\\nðŸš€ Starting BPSO for {n_iterations} Iterations...\")\n",
        "    # Using tqdm for the outer loop to track generation progress smoothly\n",
        "    for iteration in tqdm(range(n_iterations), desc=\"PSO Iterations\"):\n",
        "\n",
        "        # 1. Evaluate Fitness & Update Bests\n",
        "        for i in range(N_PARTICLES):\n",
        "            acc, _ = evaluate_subset_accuracy(positions[i], X_train, X_test, y_train, y_test)\n",
        "\n",
        "            # Update Personal Best (pbest)\n",
        "            if acc > pbest_fitness[i]:\n",
        "                pbest_fitness[i] = acc\n",
        "                pbest_positions[i] = positions[i].copy()\n",
        "\n",
        "                # Update Global Best (gbest)\n",
        "                if acc > gbest_fitness:\n",
        "                    gbest_fitness = acc\n",
        "                    gbest_position = positions[i].copy()\n",
        "\n",
        "        # 2. Update Velocity and Position\n",
        "        for i in range(N_PARTICLES):\n",
        "            r1 = np.random.rand(n_features)\n",
        "            r2 = np.random.rand(n_features)\n",
        "\n",
        "            # Calculate new velocity (Continuous update)\n",
        "            cognitive_vel = C1 * r1 * (pbest_positions[i] - positions[i])\n",
        "            social_vel = C2 * r2 * (gbest_position - positions[i])\n",
        "\n",
        "            velocities[i] = W * velocities[i] + cognitive_vel + social_vel\n",
        "\n",
        "            # Clamp velocity\n",
        "            velocities[i] = np.clip(velocities[i], -V_MAX, V_MAX)\n",
        "\n",
        "            # Calculate Sigmoid (S-shaped transfer function)\n",
        "            prob = 1.0 / (1.0 + np.exp(-velocities[i]))\n",
        "\n",
        "            # Update position (Binary update)\n",
        "            rand_nums = np.random.rand(n_features)\n",
        "            positions[i] = (rand_nums < prob).astype(int)\n",
        "\n",
        "        # Track results for this iteration\n",
        "        selected_count = int(np.sum(gbest_position))\n",
        "        history.append((iteration + 1, gbest_fitness, selected_count))\n",
        "\n",
        "        # Print update every 10 iterations or at the end\n",
        "        if (iteration + 1) % 10 == 0 or iteration == n_iterations - 1:\n",
        "            tqdm.write(f\"  Iter {iteration+1:3d}/{n_iterations} | Best Acc: {gbest_fitness:.4f} | Features: {selected_count}\")\n",
        "\n",
        "\n",
        "    # Final Evaluation\n",
        "    final_acc, final_features_count = evaluate_subset_accuracy(gbest_position, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    print(f\"\\nBPSO Run Complete ({n_iterations} Iterations)!\")\n",
        "    print(f\"Final Test Accuracy: {final_acc:.4f}\")\n",
        "    print(f\" Features Selected: {final_features_count} / {n_features}\")\n",
        "\n",
        "    return final_acc, final_features_count, gbest_position, history\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Execute Runs for 25, 50, and 100 Iterations\n",
        "# ==============================\n",
        "ITERATIONS_TO_TEST = [25, 50, 100] # Based on your Summary Table generations/iterations\n",
        "all_results = {}\n",
        "\n",
        "# Ensure data split is available from the initial part of the code\n",
        "if 'X_train' not in locals():\n",
        "    # Placeholder for the missing first part of the code execution\n",
        "    print(\"\\nERROR: Data (X_train, X_test, etc.) is not defined. Please run the setup and feature extraction part first!\")\n",
        "else:\n",
        "    for NGEN in ITERATIONS_TO_TEST:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"Starting experiment for PSO Iterations = {NGEN}\")\n",
        "        print(f\"Hyperparameters: Swarm={N_PARTICLES}, w={W}, c1={C1}, c2={C2}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        final_acc, final_features_count, best_subset, history = run_bpso(\n",
        "            NGEN, X_train, X_test, y_train, y_test, N_FEATURES\n",
        "        )\n",
        "\n",
        "        all_results[NGEN] = {\n",
        "            \"accuracy\": final_acc,\n",
        "            \"features\": final_features_count,\n",
        "            \"subset\": best_subset,\n",
        "            \"history\": history\n",
        "        }\n",
        "\n",
        "    # ==============================\n",
        "    # Summary of Results\n",
        "    # ==============================\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"### ðŸ† Final Comparison of PSO Convergence Across Iterations ###\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    for NGEN, result in all_results.items():\n",
        "        print(f\"     â–¶{NGEN} Iterations:\")\n",
        "        print(f\"     Accuracy: {result['accuracy']:.4f}\")\n",
        "        print(f\"     Features: {result['features']} / {N_FEATURES}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GnBs2VUvk8cp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}