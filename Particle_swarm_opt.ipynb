{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp3bC70IaiKP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import zipfile, os, shutil\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "1_Vu0MN8cKhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1.Mounting Google Drive...\")\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J0R-7poeXGZ",
        "outputId": "040ab089-3cda-4cd2-8a82-95253fa5576d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.Mounting Google Drive...\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AuYXUwo6fAvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/MyDrive/archive.zip\"\n",
        "extract_dir = \"/content/chest_xray\""
      ],
      "metadata": {
        "id": "pbaSTgqbe0ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"2. Extracting {zip_path}...\")\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(\"Extraction successful.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: ZIP file not found at {zip_path}. Please check the path.\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJeG1Irve4Qg",
        "outputId": "e051298a-7443-4253-8c83-4b8cad9f8248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Extracting /content/drive/MyDrive/archive.zip...\n",
            "Extraction successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = None\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    if \"train\" in dirs:\n",
        "        data_dir = root\n",
        "        break\n",
        "if data_dir is None:\n",
        "    data_dir = extract_dir\n",
        "print(\"âœ… Dataset base directory found at:\", data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbzLW_kzfcH8",
        "outputId": "fe476852-bb3e-4ff0-a2c1-b9e9e2e0f550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset base directory found at: /content/chest_xray/chest_xray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n3.  Preparing Data Loaders...\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOpnokAofl57",
        "outputId": "70769261-3667-430a-d0f7-b9b491eb394c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3.  Preparing Data Loaders...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = os.path.join(data_dir, \"train\")\n",
        "try:\n",
        "    dataset = ImageFolder(root=train_path, transform=transform)\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "except RuntimeError as e:\n",
        "     print(f\" Error loading data from {train_path}. Check if 'train' subfolder exists and contains images.\")\n",
        "     print(e)\n",
        "     exit()"
      ],
      "metadata": {
        "id": "A6qYEE3Afoky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"4. ðŸ§  Extracting features using ResNet18...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "resnet.fc = nn.Identity()  # remove final classification layer\n",
        "resnet = resnet.to(device)\n",
        "resnet.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o5-WjKRqfseQ",
        "outputId": "88b023fb-585e-44dc-bd09-4263a6949808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. ðŸ§  Extracting features using ResNet18...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 173MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = [], []"
      ],
      "metadata": {
        "id": "OqgvV9v9fzDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for imgs, lbls in tqdm(train_loader, desc=\"ðŸ” Feature Extraction Progress\"):\n",
        "        imgs = imgs.to(device)\n",
        "        out = resnet(imgs).cpu().numpy()\n",
        "        features.append(out)\n",
        "        labels.append(lbls.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCvDkm-9f2Xc",
        "outputId": "8edc821b-ec9d-4600-db1b-be7962e8d566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ðŸ” Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [01:41<00:00,  1.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = np.concatenate(features)\n",
        "labels = np.concatenate(labels)\n",
        "print(\"Feature extraction complete! Shape:\", features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKbrEHCnf5Ne",
        "outputId": "4b98c45c-5fef-42e0-d0ba-64ea9e3dbe01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction complete! Shape: (5216, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=42)\n",
        "N_FEATURES = features.shape[1]\n",
        "print(f\"Data split: Train={len(X_train)}, Test={len(X_test)}. Total features: {N_FEATURES}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi8fmuvUgC6c",
        "outputId": "a5c3b14a-75b9-4169-d741-e646da1ef559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split: Train=3912, Test=1304. Total features: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# ðŸ§¬ Binary Particle Swarm Optimization (BPSO) - 10 ITERATION TEST\n",
        "# ==============================\n",
        "\n",
        "# --- Custom Hyperparameters for Quick Test ---\n",
        "QUICK_N_ITERATIONS = 10  # Quick run limit\n",
        "QUICK_N_PARTICLES = 50   # Swarm Size: A bit smaller for speed (50 instead of 100)\n",
        "QUICK_W = 0.7            # Inertia weight: Kept at 0.7 for balance\n",
        "QUICK_C1 = 1.7           # Cognitive coefficient\n",
        "QUICK_C2 = 1.3           # Social coefficient\n",
        "QUICK_V_MAX = 6.0        # Max velocity\n",
        "\n",
        "# --- Initialization ---\n",
        "N_FEATURES = features.shape[1] # Assumes 'features' is defined from step 4\n",
        "positions = np.random.randint(0, 2, size=(QUICK_N_PARTICLES, N_FEATURES))\n",
        "velocities = np.zeros((QUICK_N_PARTICLES, N_FEATURES))\n",
        "pbest_positions = positions.copy()\n",
        "pbest_fitness = np.full(QUICK_N_PARTICLES, -np.inf)\n",
        "gbest_position = np.random.randint(0, 2, size=N_FEATURES)\n",
        "gbest_fitness = -np.inf\n",
        "\n",
        "# --- Evaluation Function (Reuse the function defined previously) ---\n",
        "# NOTE: Ensure the evaluate_subset_accuracy function (and X_train/X_test/y_train/y_test)\n",
        "# from the main script is accessible in the Colab session.\n",
        "def evaluate_subset_accuracy(individual, X_data_train, X_data_test, y_data_train, y_data_test):\n",
        "    \"\"\"Evaluates the fitness (accuracy) of a feature subset.\"\"\"\n",
        "    selected = [i for i, bit in enumerate(individual) if bit == 1]\n",
        "    if len(selected) == 0:\n",
        "        return 0.0, 0\n",
        "\n",
        "    X_sel = X_data_train[:, selected]\n",
        "    X_tst = X_data_test[:, selected]\n",
        "\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    clf = LogisticRegression(max_iter=300, random_state=42, solver='liblinear')\n",
        "    clf.fit(X_sel, y_data_train)\n",
        "    preds = clf.predict(X_tst)\n",
        "    acc = accuracy_score(y_data_test, preds)\n",
        "    return acc, len(selected)\n",
        "\n",
        "\n",
        "# --- BPSO Main Loop (10 Iterations) ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"ðŸš€ Starting QUICK BPSO Test Run: {QUICK_N_ITERATIONS} Iterations\")\n",
        "print(f\"Parameters: Swarm={QUICK_N_PARTICLES}, w={QUICK_W}, c1={QUICK_C1}, c2={QUICK_C2}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for iteration in tqdm(range(QUICK_N_ITERATIONS), desc=\"PSO Quick Iterations\"):\n",
        "    # 1. Evaluate Fitness & Update Bests\n",
        "    for i in range(QUICK_N_PARTICLES):\n",
        "        acc, _ = evaluate_subset_accuracy(positions[i], X_train, X_test, y_train, y_test)\n",
        "\n",
        "        # Update Personal Best (pbest)\n",
        "        if acc > pbest_fitness[i]:\n",
        "            pbest_fitness[i] = acc\n",
        "            pbest_positions[i] = positions[i].copy()\n",
        "\n",
        "            # Update Global Best (gbest)\n",
        "            if acc > gbest_fitness:\n",
        "                gbest_fitness = acc\n",
        "                gbest_position = positions[i].copy()\n",
        "\n",
        "    # 2. Update Velocity and Position\n",
        "    for i in range(QUICK_N_PARTICLES):\n",
        "        r1 = np.random.rand(N_FEATURES)\n",
        "        r2 = np.random.rand(N_FEATURES)\n",
        "\n",
        "        # Velocity Update\n",
        "        cognitive_vel = QUICK_C1 * r1 * (pbest_positions[i] - positions[i])\n",
        "        social_vel = QUICK_C2 * r2 * (gbest_position - positions[i])\n",
        "        velocities[i] = QUICK_W * velocities[i] + cognitive_vel + social_vel\n",
        "        velocities[i] = np.clip(velocities[i], -QUICK_V_MAX, QUICK_V_MAX)\n",
        "\n",
        "        # Position Update (Binary Sigmoid)\n",
        "        prob = 1.0 / (1.0 + np.exp(-velocities[i]))\n",
        "        rand_nums = np.random.rand(N_FEATURES)\n",
        "        positions[i] = (rand_nums < prob).astype(int)\n",
        "\n",
        "    selected_count = int(np.sum(gbest_position))\n",
        "    tqdm.write(f\"  Iter {iteration+1:2d}/{QUICK_N_ITERATIONS} | Best Acc: {gbest_fitness:.4f} | Features: {selected_count}\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# âœ… Final Results for Quick Run\n",
        "# ==============================\n",
        "final_acc, final_features_count = evaluate_subset_accuracy(gbest_position, X_train, X_test, y_train, y_test)\n",
        "\n",
        "print(\"\\nâœ… Quick BPSO Run Complete (10 Iterations)!\")\n",
        "print(f\"ðŸŽ¯ Final Test Accuracy: {final_acc:.4f}\")\n",
        "print(f\"ðŸ“ Features Selected: {final_features_count} / {N_FEATURES}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w33cYaT2lqvF",
        "outputId": "a92bdacb-db93-4437-d3bd-365d44ceada1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ðŸš€ Starting QUICK BPSO Test Run: 10 Iterations\n",
            "Parameters: Swarm=50, w=0.7, c1=1.7, c2=1.3\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations:  10%|â–ˆ         | 1/10 [00:12<01:53, 12.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  1/10 | Best Acc: 0.9701 | Features: 275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations:  20%|â–ˆâ–ˆ        | 2/10 [00:25<01:39, 12.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  2/10 | Best Acc: 0.9701 | Features: 275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:37<01:27, 12.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  3/10 | Best Acc: 0.9701 | Features: 275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:51<01:17, 12.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  4/10 | Best Acc: 0.9701 | Features: 275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:04<01:05, 13.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  5/10 | Best Acc: 0.9716 | Features: 252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:17<00:52, 13.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  6/10 | Best Acc: 0.9716 | Features: 252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:30<00:39, 13.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  7/10 | Best Acc: 0.9716 | Features: 252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:45<00:26, 13.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  8/10 | Best Acc: 0.9716 | Features: 252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:59<00:13, 13.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  9/10 | Best Acc: 0.9716 | Features: 252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Quick Iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:13<00:00, 13.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter 10/10 | Best Acc: 0.9716 | Features: 252\n",
            "\n",
            "âœ… Quick BPSO Run Complete (10 Iterations)!\n",
            "ðŸŽ¯ Final Test Accuracy: 0.9716\n",
            "ðŸ“ Features Selected: 252 / 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# ðŸ§¬ Binary Particle Swarm Optimization (BPSO) Core - **REVISED**\n",
        "# ==============================\n",
        "\n",
        "# --- PSO Hyperparameters (Based on your Summary Table) ---\n",
        "N_PARTICLES = 100   # Swarm Size: Increased for a more robust search!\n",
        "W = 0.7             # Inertia weight: Balances global and local search\n",
        "C1 = 1.7            # Cognitive coefficient: Attraction to personal best (pbest)\n",
        "C2 = 1.3            # Social coefficient: Attraction to global best (gbest)\n",
        "V_MAX = 6.0         # Maximum velocity (Kept for stability in binary PSO)\n",
        "\n",
        "# --- Evaluation Function (Remains the same, but defined again for completeness) ---\n",
        "def evaluate_subset_accuracy(individual, X_data_train, X_data_test, y_data_train, y_data_test):\n",
        "    \"\"\"Evaluates the fitness (accuracy) of a feature subset.\"\"\"\n",
        "    selected = [i for i, bit in enumerate(individual) if bit == 1]\n",
        "    if len(selected) == 0:\n",
        "        return 0.0, 0 # Return accuracy and feature count\n",
        "\n",
        "    # Select features\n",
        "    X_sel = X_data_train[:, selected]\n",
        "    X_tst = X_data_test[:, selected]\n",
        "\n",
        "    # Train and evaluate a simple classifier\n",
        "    # solver='liblinear' is generally good for small datasets and binary classification\n",
        "    # Max_iter is kept at 300 for faster evaluation within the inner loop\n",
        "    clf = LogisticRegression(max_iter=300, random_state=42, solver='liblinear')\n",
        "    clf.fit(X_sel, y_data_train)\n",
        "    preds = clf.predict(X_tst)\n",
        "    acc = accuracy_score(y_data_test, preds)\n",
        "    return acc, len(selected)\n",
        "\n",
        "\n",
        "# --- BPSO Runner Function ---\n",
        "def run_bpso(n_iterations, X_train, X_test, y_train, y_test, n_features):\n",
        "    \"\"\"Initializes and runs the BPSO algorithm for a given number of iterations.\"\"\"\n",
        "\n",
        "    # Initialization\n",
        "    # Initialize positions (particles) randomly\n",
        "    positions = np.random.randint(0, 2, size=(N_PARTICLES, n_features))\n",
        "    velocities = np.zeros((N_PARTICLES, n_features))\n",
        "\n",
        "    # Initialize personal bests (pbest)\n",
        "    pbest_positions = positions.copy()\n",
        "    pbest_fitness = np.full(N_PARTICLES, -np.inf)\n",
        "\n",
        "    # Initialize global best (gbest)\n",
        "    gbest_position = np.random.randint(0, 2, size=n_features)\n",
        "    gbest_fitness = -np.inf\n",
        "\n",
        "    # Storage for tracking results\n",
        "    history = []\n",
        "\n",
        "    print(f\"\\nðŸš€ Starting BPSO for {n_iterations} Iterations...\")\n",
        "    # Using tqdm for the outer loop to track generation progress smoothly\n",
        "    for iteration in tqdm(range(n_iterations), desc=\"PSO Iterations\"):\n",
        "\n",
        "        # 1. Evaluate Fitness & Update Bests\n",
        "        for i in range(N_PARTICLES):\n",
        "            acc, _ = evaluate_subset_accuracy(positions[i], X_train, X_test, y_train, y_test)\n",
        "\n",
        "            # Update Personal Best (pbest)\n",
        "            if acc > pbest_fitness[i]:\n",
        "                pbest_fitness[i] = acc\n",
        "                pbest_positions[i] = positions[i].copy()\n",
        "\n",
        "                # Update Global Best (gbest)\n",
        "                if acc > gbest_fitness:\n",
        "                    gbest_fitness = acc\n",
        "                    gbest_position = positions[i].copy()\n",
        "\n",
        "        # 2. Update Velocity and Position\n",
        "        for i in range(N_PARTICLES):\n",
        "            r1 = np.random.rand(n_features)\n",
        "            r2 = np.random.rand(n_features)\n",
        "\n",
        "            # Calculate new velocity (Continuous update)\n",
        "            cognitive_vel = C1 * r1 * (pbest_positions[i] - positions[i])\n",
        "            social_vel = C2 * r2 * (gbest_position - positions[i])\n",
        "\n",
        "            velocities[i] = W * velocities[i] + cognitive_vel + social_vel\n",
        "\n",
        "            # Clamp velocity\n",
        "            velocities[i] = np.clip(velocities[i], -V_MAX, V_MAX)\n",
        "\n",
        "            # Calculate Sigmoid (S-shaped transfer function)\n",
        "            prob = 1.0 / (1.0 + np.exp(-velocities[i]))\n",
        "\n",
        "            # Update position (Binary update)\n",
        "            rand_nums = np.random.rand(n_features)\n",
        "            positions[i] = (rand_nums < prob).astype(int)\n",
        "\n",
        "        # Track results for this iteration\n",
        "        selected_count = int(np.sum(gbest_position))\n",
        "        history.append((iteration + 1, gbest_fitness, selected_count))\n",
        "\n",
        "        # Print update every 10 iterations or at the end\n",
        "        if (iteration + 1) % 10 == 0 or iteration == n_iterations - 1:\n",
        "            tqdm.write(f\"  Iter {iteration+1:3d}/{n_iterations} | Best Acc: {gbest_fitness:.4f} | Features: {selected_count}\")\n",
        "\n",
        "\n",
        "    # Final Evaluation\n",
        "    final_acc, final_features_count = evaluate_subset_accuracy(gbest_position, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    print(f\"\\nâœ… BPSO Run Complete ({n_iterations} Iterations)!\")\n",
        "    print(f\"ðŸŽ¯ Final Test Accuracy: {final_acc:.4f}\")\n",
        "    print(f\"ðŸ“ Features Selected: {final_features_count} / {n_features}\")\n",
        "\n",
        "    return final_acc, final_features_count, gbest_position, history\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# ðŸ” Execute Runs for 25, 50, and 100 Iterations\n",
        "# ==============================\n",
        "ITERATIONS_TO_TEST = [25, 50, 100] # Based on your Summary Table generations/iterations\n",
        "all_results = {}\n",
        "\n",
        "# Ensure data split is available from the initial part of the code\n",
        "if 'X_train' not in locals():\n",
        "    # Placeholder for the missing first part of the code execution\n",
        "    print(\"\\nâŒ ERROR: Data (X_train, X_test, etc.) is not defined. Please run the setup and feature extraction part first!\")\n",
        "else:\n",
        "    for NGEN in ITERATIONS_TO_TEST:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"Starting experiment for PSO Iterations = {NGEN}\")\n",
        "        print(f\"Hyperparameters: Swarm={N_PARTICLES}, w={W}, c1={C1}, c2={C2}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        final_acc, final_features_count, best_subset, history = run_bpso(\n",
        "            NGEN, X_train, X_test, y_train, y_test, N_FEATURES\n",
        "        )\n",
        "\n",
        "        all_results[NGEN] = {\n",
        "            \"accuracy\": final_acc,\n",
        "            \"features\": final_features_count,\n",
        "            \"subset\": best_subset,\n",
        "            \"history\": history\n",
        "        }\n",
        "\n",
        "    # ==============================\n",
        "    # ðŸ“Š Summary of Results\n",
        "    # ==============================\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"### ðŸ† Final Comparison of PSO Convergence Across Iterations ###\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    for NGEN, result in all_results.items():\n",
        "        print(f\"  â–¶ï¸ {NGEN} Iterations:\")\n",
        "        print(f\"     Accuracy: {result['accuracy']:.4f}\")\n",
        "        print(f\"     Features: {result['features']} / {N_FEATURES}\\n\")"
      ],
      "metadata": {
        "id": "APbiKAmLg-az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b10e3a-3f1e-40d4-8afe-0349b6b53029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Starting experiment for PSO Iterations = 25\n",
            "Hyperparameters: Swarm=100, w=0.7, c1=1.7, c2=1.3\n",
            "======================================================================\n",
            "\n",
            "ðŸš€ Starting BPSO for 25 Iterations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [04:18<06:29, 25.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  10/25 | Best Acc: 0.9739 | Features: 270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [08:29<02:04, 24.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  20/25 | Best Acc: 0.9747 | Features: 258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [10:33<00:00, 25.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  25/25 | Best Acc: 0.9747 | Features: 258\n",
            "\n",
            "âœ… BPSO Run Complete (25 Iterations)!\n",
            "ðŸŽ¯ Final Test Accuracy: 0.9747\n",
            "ðŸ“ Features Selected: 258 / 512\n",
            "\n",
            "======================================================================\n",
            "Starting experiment for PSO Iterations = 50\n",
            "Hyperparameters: Swarm=100, w=0.7, c1=1.7, c2=1.3\n",
            "======================================================================\n",
            "\n",
            "ðŸš€ Starting BPSO for 50 Iterations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  20%|â–ˆâ–ˆ        | 10/50 [04:10<16:38, 24.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  10/50 | Best Acc: 0.9747 | Features: 262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [08:20<12:27, 24.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  20/50 | Best Acc: 0.9747 | Features: 262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [12:31<08:19, 24.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  30/50 | Best Acc: 0.9747 | Features: 262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [16:38<04:04, 24.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  40/50 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [20:42<00:00, 24.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  50/50 | Best Acc: 0.9755 | Features: 242\n",
            "\n",
            "âœ… BPSO Run Complete (50 Iterations)!\n",
            "ðŸŽ¯ Final Test Accuracy: 0.9755\n",
            "ðŸ“ Features Selected: 242 / 512\n",
            "\n",
            "======================================================================\n",
            "Starting experiment for PSO Iterations = 100\n",
            "Hyperparameters: Swarm=100, w=0.7, c1=1.7, c2=1.3\n",
            "======================================================================\n",
            "\n",
            "ðŸš€ Starting BPSO for 100 Iterations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  10%|â–ˆ         | 10/100 [04:06<36:38, 24.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  10/100 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  20%|â–ˆâ–ˆ        | 20/100 [08:08<31:57, 23.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  20/100 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [12:12<28:19, 24.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  30/100 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [16:14<24:05, 24.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  40/100 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [20:16<20:13, 24.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  50/100 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [24:19<16:15, 24.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  60/100 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [28:21<12:06, 24.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  70/100 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [32:25<08:06, 24.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  80/100 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [36:28<04:02, 24.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter  90/100 | Best Acc: 0.9755 | Features: 242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PSO Iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [40:31<00:00, 24.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Iter 100/100 | Best Acc: 0.9770 | Features: 258\n",
            "\n",
            "âœ… BPSO Run Complete (100 Iterations)!\n",
            "ðŸŽ¯ Final Test Accuracy: 0.9770\n",
            "ðŸ“ Features Selected: 258 / 512\n",
            "\n",
            "######################################################################\n",
            "### ðŸ† Final Comparison of PSO Convergence Across Iterations ###\n",
            "######################################################################\n",
            "  â–¶ï¸ 25 Iterations:\n",
            "     Accuracy: 0.9747\n",
            "     Features: 258 / 512\n",
            "\n",
            "  â–¶ï¸ 50 Iterations:\n",
            "     Accuracy: 0.9755\n",
            "     Features: 242 / 512\n",
            "\n",
            "  â–¶ï¸ 100 Iterations:\n",
            "     Accuracy: 0.9770\n",
            "     Features: 258 / 512\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GnBs2VUvk8cp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}